{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e179124-5c13-4864-b2a1-a7e2b5a1324d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# please save me I want to have some sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Model\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b7d13b-5711-4151-ac1a-66147507809b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. authenticate and experiment\n",
    "\n",
    "# Authenticate with Azure Managed Identity automatically\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Connect to Azure ML\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"0a601663-6ddf-4fa7-9c07-cf51438535af\",\n",
    "    resource_group_name=\"diplomka\",\n",
    "    workspace_name=\"AML-diplomka\"\n",
    ")\n",
    "\n",
    "print(\"Connected to Azure ML workspace\")\n",
    "\n",
    "# initiate ml flow experiment for saving training data\n",
    "mlflow.set_experiment(\"/Users/bulanec123@gmail.com/irradiance_retrain_experiment\")\n",
    "\n",
    "model_name = \"irradiance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46255eb6-036e-439e-a2ab-b4759ba726a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. functions (all logic to use)\n",
    "\n",
    "def load_latest_model(model_name, ml_client):\n",
    "    models = list(ml_client.models.list(name=model_name))\n",
    "\n",
    "    # get latest model was not able to figure it out from documentation done by chat gpt\n",
    "    latest_model = sorted(models, key=lambda m: m.creation_context.created_at, reverse=True)[0]\n",
    "\n",
    "    print(f\"Latest model: {latest_model.name}, version: {latest_model.version}\")\n",
    "\n",
    "    # get the latest model to dfbs so we can access it for training we will delete if afterwards\n",
    "    download_path = \"./local_model\"\n",
    "    ml_client.models.download(\n",
    "        name=latest_model.name,\n",
    "        version=latest_model.version,\n",
    "        download_path=download_path\n",
    "    )\n",
    "    model_folder = os.path.join(download_path, latest_model.name)\n",
    "    keras_model_path = None\n",
    "\n",
    "    # was not able to find it with path sometimes so I go through each one\n",
    "    for root, dirs, files in os.walk(model_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".keras\"):\n",
    "                keras_model_path = os.path.join(root, file)\n",
    "                break\n",
    "\n",
    "\n",
    "    model = tf.keras.models.load_model(keras_model_path)\n",
    "    print(f\"Model loaded\")\n",
    "    return model, latest_model\n",
    "\n",
    "def load_all_gold_tables():\n",
    "    tables = spark.catalog.listTables(\"model_workspace.gold\")\n",
    "    table_names = [t.name for t in tables if t.tableType == \"MANAGED\"]\n",
    "    table_names.sort()\n",
    "    print(f\"najdene: {table_names}\")\n",
    "\n",
    "    train_dfs, val_dfs = [], []\n",
    "\n",
    "    for idx, table_name in enumerate(table_names):\n",
    "        df = spark.read.table(f\"model_workspace.gold.{table_name}\").toPandas()\n",
    "        \n",
    "        # if 'UnixTime' in df.columns:\n",
    "        #     df = df.drop(columns=['UnixTime'])\n",
    "\n",
    "        # Currently not checking for new data but importing all available data in gold tables and retraining the model\n",
    "        # This was done in this way because we do not have more available data for training and this solution is proof of concept \n",
    "        # not a production environment, this was done for testing as well\n",
    "\n",
    "        if idx < 3:\n",
    "            train_dfs.append(df)\n",
    "        else:\n",
    "            val_dfs.append(df)\n",
    "\n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "    val_df = pd.concat(val_dfs, ignore_index=True)\n",
    "    print(f\"train : {len(train_df)}   val : {len(val_df)}\")\n",
    "    return train_df, val_df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "    return df_scaled, scaler\n",
    "\n",
    "def create_sequences(df_scaled, target_column_name=\"Irradiance\", seq_length=48, use_multivariate=True):\n",
    "    data = df_scaled.values\n",
    "    target_column = df_scaled.columns.get_loc(target_column_name)\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        if not use_multivariate:\n",
    "            X.append(data[i:i + seq_length, target_column].reshape(-1, 1))\n",
    "        else:\n",
    "            X.append(data[i:i + seq_length, :])\n",
    "        y.append(data[i + seq_length, target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def fine_tune_model(model, X_train, y_train, X_val, y_val, learning_rate=0.00000001, batch_size=32, epochs=5):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_val, y_val),callbacks=[early_stopping],verbose=1)\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return model, val_loss\n",
    "\n",
    "def compare_and_register(model_name, ml_client, model, val_loss, old_model, X_val, y_val, scaler, df_scaled_val):\n",
    "    y_pred_scaled = model.predict(X_val)\n",
    "    target_index = df_scaled_val.columns.get_loc(\"Irradiance\")\n",
    "\n",
    "    # need to inverse to get real data for valdating it\n",
    "    y_val_actual = inverse_transform_target(y_val, scaler, df_scaled_val.shape[1], target_index)\n",
    "    y_pred_actual = inverse_transform_target(y_pred_scaled, scaler, df_scaled_val.shape[1], target_index)\n",
    "\n",
    "   \n",
    "    fine_tuned_r2 = r2_score(y_val_actual, y_pred_actual)\n",
    "    print(f\"fine-tuned   R2: {fine_tuned_r2:.4f}\")\n",
    "\n",
    "    # get old metrics from tags that we inseted in the model in Azure ML we can see it in picture in thesis as well\n",
    "    old_val_loss = old_model.tags.get(\"val_loss\")\n",
    "    old_r2 = old_model.tags.get(\"r2_score\")\n",
    "    old_val_loss = float(old_val_loss)\n",
    "    old_r2 = float(old_r2)\n",
    "        \n",
    "\n",
    "    is_better = False\n",
    "    if (old_val_loss is None) or (val_loss < old_val_loss):\n",
    "        is_better = True\n",
    "    elif (old_r2 is None) or (fine_tuned_r2 > old_r2):\n",
    "        is_better = True\n",
    "   \n",
    "\n",
    "    # register model if its prediction was better then old one\n",
    "\n",
    "    if is_better:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            model_save_path = os.path.join(tmpdir, \"model.keras\")\n",
    "            model.save(model_save_path)\n",
    "\n",
    "            registered_model = Model(\n",
    "                path=tmpdir,\n",
    "                name=model_name,\n",
    "                description=\"Fine-tuned model with improved validation loss or R2\",\n",
    "                tags={\n",
    "                    \"val_loss\": str(val_loss),\n",
    "                    \"r2_score\": str(fine_tuned_r2)\n",
    "                }\n",
    "            )\n",
    "            registered_model = ml_client.models.create_or_update(registered_model)\n",
    "            print(f\"registered: {registered_model.name}, version: {registered_model.version}\")\n",
    "    else:\n",
    "        print(\"did not improve\")\n",
    "\n",
    "def inverse_transform_target(scaled_target, scaler, n_features, target_index):\n",
    "    zero_filled = np.zeros((len(scaled_target), n_features))\n",
    "    zero_filled[:, target_index] = scaled_target.flatten()\n",
    "    return scaler.inverse_transform(zero_filled)[:, target_index]\n",
    "\n",
    "def plot_and_log_predictions(X_val, y_val, model, scaler, df_scaled_val):\n",
    "    y_pred_scaled = model.predict(X_val)\n",
    "    target_index = df_scaled_val.columns.get_loc(\"Irradiance\")\n",
    "\n",
    "    y_val_actual = inverse_transform_target(y_val, scaler, df_scaled_val.shape[1], target_index)\n",
    "    y_pred_actual = inverse_transform_target(y_pred_scaled, scaler, df_scaled_val.shape[1], target_index)\n",
    "\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"Actual Irradiance\": y_val_actual,\n",
    "        \"Predicted Irradiance\": y_pred_actual\n",
    "    })\n",
    "\n",
    "\n",
    "    fine_tuned_r2 = r2_score(y_val_actual, y_pred_actual)\n",
    "    fine_tuned_mae = mean_absolute_error(y_val_actual, y_pred_actual)\n",
    "    fine_tuned_rmse = np.sqrt(mean_squared_error(y_val_actual, y_pred_actual))\n",
    "\n",
    "    # create a temp folder\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # --- Save plot ---\n",
    "        plots_dir = os.path.join(tmpdir, \"plots\")\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        plot_path = os.path.join(plots_dir, \"actual_vs_predicted.png\")\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(predictions_df[\"Actual Irradiance\"], label=\"Actual\", alpha=0.7)\n",
    "        plt.plot(predictions_df[\"Predicted Irradiance\"], label=\"Predicted\", alpha=0.7)\n",
    "        plt.title(\"Validation Set: Actual vs Predicted Irradiance\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Irradiance\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
    "        print(f\"plot saved\")\n",
    "\n",
    "        # save metrics to experiment\n",
    "        metrics_dir = os.path.join(tmpdir, \"metrics\")\n",
    "        os.makedirs(metrics_dir, exist_ok=True)\n",
    "        metrics_path = os.path.join(metrics_dir, \"metrics.json\")\n",
    "\n",
    "        metrics = {\n",
    "            \"r2_score\": fine_tuned_r2,\n",
    "            \"mae\": fine_tuned_mae,\n",
    "            \"rmse\": fine_tuned_rmse\n",
    "        }\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "        mlflow.log_artifact(metrics_path, artifact_path=\"metrics\")\n",
    "        print(f\"metrics file saved\")\n",
    "\n",
    "    mlflow.log_metric(\"r2_score\", fine_tuned_r2)\n",
    "    mlflow.log_metric(\"mae\", fine_tuned_mae)\n",
    "    mlflow.log_metric(\"rmse\", fine_tuned_rmse)\n",
    "\n",
    "    print(f\"R2: {fine_tuned_r2:.4f}\")\n",
    "    print(f\"MAE: {fine_tuned_mae:.4f}\")\n",
    "    print(f\"RMSE: {fine_tuned_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59222d9-09af-49f0-a7b8-d102bdbe9673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. main code of this task\n",
    "\n",
    "with mlflow.start_run(run_name=\"Retrain-FineTune-Register\"):\n",
    "\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.set_tag(\"pipeline\", \"automated_retrain\")\n",
    "\n",
    "    model, old_model = load_latest_model(model_name, ml_client)\n",
    "    train_df, val_df = load_all_gold_tables()\n",
    "\n",
    "    train_scaled, scaler = preprocess_data(train_df)\n",
    "    val_scaled, _ = preprocess_data(val_df)\n",
    "\n",
    "    X_train, y_train = create_sequences(train_scaled)\n",
    "    X_val, y_val = create_sequences(val_scaled)\n",
    "\n",
    "    fine_tuned_model, fine_tuned_val_loss = fine_tune_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    mlflow.log_metric(\"fine_tuned_val_loss\", fine_tuned_val_loss)\n",
    "\n",
    "    compare_and_register(model_name, ml_client, fine_tuned_model, fine_tuned_val_loss, old_model, X_val, y_val, scaler, val_scaled)\n",
    "    plot_and_log_predictions(X_val, y_val, fine_tuned_model, scaler, val_scaled)\n",
    "\n",
    "print(\"workflow is finished\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "automated training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
