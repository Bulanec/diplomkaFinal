{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paramiko\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# init catalog\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SFTP to Unity Catalog Bronze\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# sftp\n",
    "hostname = '147.175.106.193'\n",
    "port = 2020\n",
    "username = 'xbachorik'\n",
    "password = 'AzureDatabricks1523'\n",
    "remote_dir = '/home/xbachorik/data/weeks'\n",
    "\n",
    "# dfbs\n",
    "local_dir = '/dbfs/local_data/bronze'     \n",
    "spark_dir = 'dbfs:/local_data/bronze'     \n",
    "\n",
    "# write\n",
    "catalog = \"model_workspace\"\n",
    "schema = \"bronze\"\n",
    "\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# connect\n",
    "transport = paramiko.Transport((hostname, port))\n",
    "transport.connect(username=username, password=password)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "sftp.chdir(remote_dir)\n",
    "\n",
    "# Preprocess each file in root directory and read time stamp\n",
    "for file in sftp.listdir():\n",
    "    if file.endswith(\".csv\"):\n",
    "\n",
    "        table_name = file.replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "        full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "\n",
    "\n",
    "        if spark._jsparkSession.catalog().tableExists(full_table_name):\n",
    "            print(f\"Skipping:{full_table_name}\")\n",
    "            continue\n",
    "\n",
    "        # ingest and save to dfbs amd after that save to schema\n",
    "        local_path = os.path.join(local_dir, file)\n",
    "        try:\n",
    "            sftp.get(file, local_path)\n",
    "            print(f\"Downloaded:{file}\")\n",
    "\n",
    "            spark_path = f\"{spark_dir}/{file}\"\n",
    "\n",
    "            df = spark.read.option(\"header\", True).csv(spark_path)\n",
    "            print(f\"Loaded: {file}\")\n",
    "\n",
    "            # Regex to check if the filename contains a date generated by chat gpt\n",
    "            match = re.search(r'_(\\d{4}-\\d{2}-\\d{2})_to_', file)\n",
    "            if match:\n",
    "                temp = match.group(1)\n",
    "                parsed_datetime = datetime.strptime(temp, \"%Y-%m-%d\")\n",
    "                df = df.withColumn(\"DateTime\", lit(parsed_datetime))\n",
    "            else:\n",
    "                print(f\"Skipping {file}: cant parse date\")\n",
    "                continue\n",
    "\n",
    "            def clean_column_name(name):\n",
    "                return name.strip().replace(\" \", \"_\").replace(\":\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "            df = df.toDF(*[clean_column_name(c) for c in df.columns])\n",
    "\n",
    "            # save to bronze schema\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            print(f\"written to schema:{full_table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"err {file}: {e}\")\n",
    "\n",
    "\n",
    "sftp.close()\n",
    "transport.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
