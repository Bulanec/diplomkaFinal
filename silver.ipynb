{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67759dd1-d76a-4a78-ae95-c66b8b60f336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, year, month, dayofyear, hour, day\n",
    "from pyspark.sql.types import TimestampType\n",
    "from astral.sun import sun\n",
    "from astral import Observer, LocationInfo\n",
    "import pytz\n",
    "import pandas as pd\n",
    "\n",
    "# this needs to be executed to access ACID functionality and manipulate with tables in unity catalog\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# new feature from astral library\n",
    "location = LocationInfo(name=\"Alpnach\", region=\"Switzerland\", timezone=\"Europe/Zurich\", latitude=46.94, longitude=8.28)\n",
    "observer = Observer(latitude=46.94, longitude=8.28, elevation=450)\n",
    "tz = pytz.timezone(location.timezone)\n",
    "\n",
    "\n",
    "# get all bronye tables by name month* \n",
    "bronze_tables = spark.sql(\"SHOW TABLES IN model_workspace.bronze\") \\\n",
    "    .filter(\"tableName LIKE 'month%'\") \\\n",
    "    .select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "# check if the silver table is saved in silver schema if not it is new data and it should be saved\n",
    "for table_name in bronze_tables:\n",
    "\n",
    "    bronze_table = f\"model_workspace.bronze.{table_name}\"\n",
    "    silver_table = f\"model_workspace.silver.{table_name}_features\"\n",
    "    if spark._jsparkSession.catalog().tableExists(silver_table):\n",
    "        print(f\"skipping (already exists):{silver_table}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing:{bronze_table} to {silver_table}\")\n",
    "\n",
    "    try:\n",
    "        df = spark.read.table(bronze_table)\n",
    "\n",
    "        if \"ParsedDateTime\" not in df.columns:\n",
    "            print(f\"skipping {table_name}   missing ParsedDateTime column\")\n",
    "            continue\n",
    "\n",
    "        # had issue with wrong column name when uploading data to server this solved it\n",
    "        df = df.withColumn(\"DateTime\", col(\"ParsedDateTime\").cast(TimestampType()))\n",
    "        df = df.drop(\"ParsedDateTime\")\n",
    "\n",
    "        # extracting time features to manipulate them later\n",
    "        df = df.dropna(subset=[\"DateTime\", \"Irradiance\"])\n",
    "        df = df.withColumn(\"UnixTime\", unix_timestamp(\"DateTime\")) \\\n",
    "               .withColumn(\"Month\", month(\"DateTime\")) \\\n",
    "               .withColumn(\"DayOfTheYear\", dayofyear(\"DateTime\")) \\\n",
    "               .withColumn(\"Day\", day(\"DateTime\")) \\\n",
    "               .withColumn(\"Hour\", hour(\"DateTime\")) \\\n",
    "               .withColumn(\"Year\", year(\"DateTime\"))\n",
    "\n",
    "\n",
    "        pandas_df = df.toPandas()\n",
    "        pandas_df['DateTime'] = pd.to_datetime(pandas_df['DateTime'])\n",
    "        pandas_df['Date'] = pandas_df['DateTime'].dt.date\n",
    "\n",
    "        # compuute DayLength once per day and not for every row saving time and compute cost\n",
    "        def compute_daylength(date):\n",
    "            try:\n",
    "                s = sun(observer, date=date, tzinfo=tz)\n",
    "                return (s['sunset'] - s['sunrise']).seconds / 3600 # in hours because model wont wrk well with seconds, too big values\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # done with chatgpt couldnt make it work with my code ... this is cleaner as well\n",
    "        daylength_map = {d: compute_daylength(d) for d in pandas_df['Date'].drop_duplicates()}\n",
    "        pandas_df['DayLength'] = pandas_df['Date'].map(daylength_map)\n",
    "\n",
    "\n",
    "        pandas_df.drop(columns=[\"Year\", \"Date\", \"PictureName\"], inplace=True)\n",
    "\n",
    "        #  save  to silver schema\n",
    "        silver_df = spark.createDataFrame(pandas_df)\n",
    "        silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "\n",
    "        print(f\"wrote silver table:{silver_table}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"err {table_name}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
